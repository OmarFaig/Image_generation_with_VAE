{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import utils\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Karthik11232/human_face_generation\")\n",
    "#split the dataset \n",
    "train_test_set = ds['train'].train_test_split(test_size=0.2)\n",
    "#print(train_test_set)\n",
    "train_val_set = train_test_set['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': train_test_set['train'],\n",
    "    'test': train_test_set['test'],\n",
    "    'validation': train_val_set['test']\n",
    "})\n",
    "#print (ds)\n",
    "print(ds.shape)\n",
    "print(train_test_set.shape)\n",
    "print(train_val_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets to disk\n",
    "output_dir = './human_face_generation_split/raw'\n",
    "if not os.path.exists(output_dir):  \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "# Save each split to disk\n",
    "    utils.save_images(ds['train'], 'train',output_dir)\n",
    "    utils.save_images(ds['validation'], 'validation')\n",
    "    utils.save_images(ds['test'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 15 random images in 3 rows and 5 columns\n",
    "utils.plot_random_images(dataset_path='human_face_generation_split/raw/train', num_images=15, rows=3, cols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepoccessing \n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),  # Resize to a specific size if needed\n",
    "    transforms.ToTensor(),          # Convert to tensor and normalize\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "preprocessed_output_dir = './human_face_generation_split/preprocessed'\n",
    "os.makedirs(preprocessed_output_dir, exist_ok=True)\n",
    "if not os.path.exists(preprocessed_output_dir):\n",
    "# Save preprocessed training images\n",
    "\n",
    "    utils.save_images(ds['train'], 'train', preprocessed_output_dir,preprocess=preprocess)\n",
    "    utils.plot_random_images(os.path.join(preprocessed_output_dir, 'train'), num_images=15, rows=3, cols=5)\n",
    "    img = Image.open(\"human_face_generation_split/preprocessed/train/image_2110.png\")\n",
    "    print(img.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first implement a simple Autoencoder with Linear layers\n",
    "class Autoencoder_linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.size_hidden_layers = 512 \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(40000,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,self.size_hidden_layers),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.size_hidden_layers,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,40000),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "   # init_weights(self.encoder)\n",
    "    def forward(self,x):\n",
    "        encoded=self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded,decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,image_folder):\n",
    "        self.image_folder=image_folder\n",
    "        self.images = os.listdir (image_folder)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((200, 200)),  # Resize images to 200x200\n",
    "            transforms.ToTensor()           # Convert images to PyTorch tensors\n",
    "        ])\n",
    "    def __getitem__(self,idx):\n",
    "        image_file = self.images[idx]\n",
    "        image = Image.open((self.image_folder +\"/\"+ image_file))\n",
    "        image = self.transform(image)\n",
    "        #image = np.array(image).reshape(-1,200*200) for lin autoencoder\n",
    "\n",
    "\n",
    "        image = torch.Tensor(image)\n",
    "       # print(image.shape)\n",
    "\n",
    "       # print (image.shape)\n",
    "        return image\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "dataset_preprocessed = ImageDataset(\"human_face_generation_split/preprocessed/train\")\n",
    "dataset_val = ImageDataset(\"human_face_generation_split/raw/validation\")\n",
    "dataset_test = ImageDataset(\"human_face_generation_split/raw/test\")\n",
    "show_dataset = ImageDataset(\"human_face_generation_split/show_imgs/train\")\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(dataset_preprocessed,batch_size=64,shuffle=True)\n",
    "val_data = torch.utils.data.DataLoader(dataset_val)\n",
    "test_data = torch.utils.data.DataLoader(dataset_test)\n",
    "show_data =torch.utils.data.DataLoader(show_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckp_path=\"human_face_generation_split_data{}.ckp\"\n",
    "crt_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "dir_ckpt = os.path.join(\"human_face_generation_split/\",str(crt_time))\n",
    "os.mkdir(dir_ckpt)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "linear_auto = Autoencoder_linear()\n",
    "print(linear_auto)\n",
    "linear_auto.to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(linear_auto.parameters(), lr=1e-4)\n",
    "#scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "epochs =  100\n",
    "#training\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    with tqdm(train_data, unit=\"batch\", total=len(train_data), desc=f\"Epoch {epoch+1}/{epochs}\") as tepoch:\n",
    "        for batch_idx, data in enumerate(tepoch):\n",
    "            data = data.to(device)\n",
    "            encoded, decoded = linear_auto(data)\n",
    "            loss = loss_function(decoded, data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # Set the postfix with current loss\n",
    "            tepoch.set_postfix(loss=total_loss / ((batch_idx + 1) * data.size(0)))\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            #print(decoded.shape)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        linear_auto.eval()  # Set the model to evaluation mode\n",
    "        ckpt_file = os.path.join(dir_ckpt, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': linear_auto.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss,\n",
    "            \n",
    "        },ckpt_file)\n",
    "    epoch_loss = total_loss / len(train_data.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} : loss = {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3 , out_channels = 32, kernel_size = 2 ,stride = 2),\n",
    "            nn.ReLU(),# possibly add max pooling\n",
    "            nn.Conv2d(in_channels = 32,out_channels = 64, kernel_size = 2, stride = 2),\n",
    "            nn.ReLU()\n",
    "        )    \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = 64,out_channels=32,kernel_size=2,stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels = 32 , out_channels = 3 ,kernel_size=2,stride =2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "crt_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "dir_ckpt = os.path.join(\"human_face_generation_split/\",str(crt_time)+\"conv\")\n",
    "os.mkdir(dir_ckpt)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "autoencoder_conv = Autoencoder_conv().to(device)  # Move the model to GPU\n",
    "print(autoencoder_conv)\n",
    "#linear_auto = Autoencoder_linear()\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder_conv.parameters(), lr=1e-4)\n",
    "#scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "epochs =  100\n",
    "#training\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    with tqdm(train_data, unit=\"batch\", total=len(train_data), desc=f\"Epoch {epoch+1}/{epochs}\") as tepoch:\n",
    "        for batch_idx, data in enumerate(tepoch):\n",
    "            #print(data.shape)  # Add this to debug the shape\n",
    "\n",
    "            data = data.to(device)\n",
    "            encoded, decoded = autoencoder_conv(data)\n",
    "            loss = loss_function(decoded, data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # Set the postfix with current loss\n",
    "            tepoch.set_postfix(loss=total_loss / ((batch_idx + 1) * data.size(0)))\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            #print(decoded.shape)\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        autoencoder_conv.eval()  # Set the model to evaluation mode\n",
    "        ckpt_file = os.path.join(dir_ckpt, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': autoencoder_conv.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss,\n",
    "            \n",
    "        },ckpt_file)\n",
    "    epoch_loss = total_loss / len(train_data.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} : loss = {epoch_loss:.4f}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder VAE\n",
    "class AutoencoderVAE:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def encoder(self, data):\n",
    "        # Define the encoder architecture\n",
    "        # The output shape should be (batch_size, latent_dim, 4, 4)\n",
    "        pass\n",
    "    def decoder(self, data):\n",
    "        # Define the decoder architecture\n",
    "        # The output shape should be (batch_size, 3, 200, 200)\n",
    "        pass\n",
    "    def reparametrize(self, data):\n",
    "        # Implement reparameterization trick to sample z from the latent space\n",
    "        pass\n",
    "    def forward(self, data):\n",
    "        # Implement the forward pass through the VAE\n",
    "        # Return the reconstructed data, the latent mean, and the latent log variance\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
